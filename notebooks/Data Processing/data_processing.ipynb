{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a02e8c2-ca96-4149-b98c-273fd2121722",
   "metadata": {},
   "source": [
    "# 1) Define all file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4b37d7fb-e33b-495c-9223-51bc47240c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "2021734d-6752-4f00-bae6-e9ee190ab8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base folder where your “data” directory lives\n",
    "base_dir = r\"C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f32e559a-266c-47e8-bcac-33f66af46aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the auxiliary CSV files\n",
    "csv_paths = {\n",
    "    \"btc_eth_price\":           os.path.join(base_dir, \"btc_eth_price.csv\"),\n",
    "    \"holders\":                 os.path.join(base_dir, \"holders.csv\"),\n",
    "    \"network_tx\":              os.path.join(base_dir, \"network_transaction_counts.csv\"),\n",
    "    \"new_token_accounts\":      os.path.join(base_dir, \"new_token_accounts.csv\"),\n",
    "    \"sol_price\":               os.path.join(base_dir, \"sol_price.csv\"),\n",
    "    \"token_transfers\":         os.path.join(base_dir, \"token_transfers.csv\"),\n",
    "    \"tvl\":                     os.path.join(base_dir, \"tvl.csv\"),\n",
    "    \"spl_instructions\":        os.path.join(base_dir, \"SPL Program Instructions.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "df5a00d5-fff3-490c-be92-aee636ff6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the SQLite database\n",
    "db_path = os.path.join(base_dir, \"solana_data_12h.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a1c9c1fc-d4b1-470c-882e-e929aac25da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 23 token CSV files. Example:\n",
      "    $COLLAT.csv\n",
      "    $michi.csv\n",
      "    $WIF.csv\n",
      "    ALCH.csv\n",
      "    AVA.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pattern to capture all token OHLCV CSVs under “data/tokens/”\n",
    "token_csv_pattern = os.path.join(base_dir, \"tokens\", \"*.csv\")\n",
    "token_files = glob.glob(token_csv_pattern)\n",
    "\n",
    "print(f\"Discovered {len(token_files)} token CSV files. Example:\")\n",
    "for f in token_files[:5]:\n",
    "    print(\"   \", os.path.basename(f))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764c2c6-c4ab-41b7-a938-9d733e54f7d9",
   "metadata": {},
   "source": [
    "# 2) Load and preprocess each auxiliary CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "fbe908f3-abbf-4315-90f7-9afe16f08ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store each DataFrame in a dictionary called `dfs`, keyed by a short name.\n",
    "dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6b87a3a5-3969-404c-9cea-184617d19ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1: Load btc_eth_price.csv ---\n",
    "df_btceth = pd.read_csv(csv_paths[\"btc_eth_price\"])\n",
    "if \"timestamp\" not in df_btceth.columns:\n",
    "    for col in [\"time\", \"date\", \"datetime\"]:\n",
    "        if col in df_btceth.columns:\n",
    "            df_btceth.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "            break\n",
    "df_btceth[\"timestamp\"] = pd.to_datetime(df_btceth[\"timestamp\"])\n",
    "dfs[\"btc_eth_price\"] = df_btceth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "6398d4dd-cb3f-4d64-9242-09b86e5266e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2: Load holders.csv (KEEP holder_count) ---\n",
    "df_holders = pd.read_csv(csv_paths[\"holders\"])\n",
    "if \"timestamp\" not in df_holders.columns:\n",
    "    for col in [\"time\", \"date\", \"datetime\"]:\n",
    "        if col in df_holders.columns:\n",
    "            df_holders.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "            break\n",
    "df_holders[\"timestamp\"] = pd.to_datetime(df_holders[\"timestamp\"])\n",
    "# Rename token_mint → token (for consistency)\n",
    "if \"token_mint\" in df_holders.columns:\n",
    "    df_holders.rename(columns={\"token_mint\": \"token\"}, inplace=True)\n",
    "# We do NOT drop holder_count—this is needed\n",
    "dfs[\"holders\"] = df_holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b487adc2-6591-4181-b02b-4fa05cae30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.3: Load network_transaction_counts.csv ---\n",
    "df_net = pd.read_csv(csv_paths[\"network_tx\"])\n",
    "if \"timestamp\" not in df_net.columns:\n",
    "    for col in [\"bucket_start\", \"date\", \"datetime\"]:\n",
    "        if col in df_net.columns:\n",
    "            df_net.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "            break\n",
    "df_net[\"timestamp\"] = pd.to_datetime(df_net[\"timestamp\"])\n",
    "dfs[\"network_tx\"] = df_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1b81b527-f7be-4298-b2ad-5a43a6224f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.4: Load new_token_accounts.csv ---\n",
    "df_newacc = pd.read_csv(csv_paths[\"new_token_accounts\"])\n",
    "# Identify timestamp column as \"bucket_start\"\n",
    "if \"timestamp\" not in df_newacc.columns:\n",
    "    if \"bucket_start\" in df_newacc.columns:\n",
    "        df_newacc.rename(columns={\"bucket_start\": \"timestamp\"}, inplace=True)\n",
    "    else:\n",
    "        for col in [\"date\", \"datetime\"]:\n",
    "            if col in df_newacc.columns:\n",
    "                df_newacc.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "                break\n",
    "df_newacc[\"timestamp\"] = pd.to_datetime(df_newacc[\"timestamp\"])\n",
    "# Rename token_mint_address → token (for consistency)\n",
    "if \"token_mint_address\" in df_newacc.columns:\n",
    "    df_newacc.rename(columns={\"token_mint_address\": \"token\"}, inplace=True)\n",
    "# Keep the new_token_accounts column as-is\n",
    "dfs[\"new_token_accounts\"] = df_newacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "694b6fc0-2785-4fdb-8a1f-35f0dc1f0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.5: Load sol_price.csv ---\n",
    "df_sol = pd.read_csv(csv_paths[\"sol_price\"])\n",
    "if \"timestamp\" not in df_sol.columns:\n",
    "    for col in [\"time\", \"date\", \"datetime\"]:\n",
    "        if col in df_sol.columns:\n",
    "            df_sol.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "            break\n",
    "df_sol[\"timestamp\"] = pd.to_datetime(df_sol[\"timestamp\"])\n",
    "dfs[\"sol_price\"] = df_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "0bd2c869-46ff-48b1-a185-f9fe4ea16d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.6: Load token_transfers.csv ---\n",
    "df_transfers = pd.read_csv(csv_paths[\"token_transfers\"])\n",
    "# Identify timestamp column as \"bucket_start\"\n",
    "if \"timestamp\" not in df_transfers.columns:\n",
    "    if \"bucket_start\" in df_transfers.columns:\n",
    "        df_transfers.rename(columns={\"bucket_start\": \"timestamp\"}, inplace=True)\n",
    "    else:\n",
    "        for col in [\"time\", \"date\", \"datetime\"]:\n",
    "            if col in df_transfers.columns:\n",
    "                df_transfers.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "                break\n",
    "df_transfers[\"timestamp\"] = pd.to_datetime(df_transfers[\"timestamp\"])\n",
    "# Rename token_mint_address → token (for consistency)\n",
    "if \"token_mint_address\" in df_transfers.columns:\n",
    "    df_transfers.rename(columns={\"token_mint_address\": \"token\"}, inplace=True)\n",
    "# Drop extra columns we won’t use (e.g., unique_senders, unique_receivers)\n",
    "for col in [\"unique_senders\", \"unique_receivers\"]:\n",
    "    if col in df_transfers.columns:\n",
    "        df_transfers.drop(columns=[col], inplace=True)\n",
    "# Keep transfer_count\n",
    "dfs[\"transfers\"] = df_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "b2f22623-1208-4c9b-b872-390dc2929c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.7: Load tvl.csv ---\n",
    "df_tvl = pd.read_csv(csv_paths[\"tvl\"])\n",
    "if \"timestamp\" not in df_tvl.columns:\n",
    "    for col in [\"bucket_start\", \"date\", \"datetime\"]:\n",
    "        if col in df_tvl.columns:\n",
    "            df_tvl.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "            break\n",
    "df_tvl[\"timestamp\"] = pd.to_datetime(df_tvl[\"timestamp\"])\n",
    "dfs[\"tvl\"] = df_tvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5de5c79f-83a9-4643-8d3f-b62840adcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.8: Load SPL Program Instructions (optional) ---\n",
    "df_spl = pd.read_csv(csv_paths[\"spl_instructions\"])\n",
    "if \"timestamp\" not in df_spl.columns:\n",
    "    for col in [\"bucket_start\", \"date\", \"datetime\"]:\n",
    "        if col in df_spl.columns:\n",
    "            df_spl.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "            break\n",
    "df_spl[\"timestamp\"] = pd.to_datetime(df_spl[\"timestamp\"])\n",
    "dfs[\"spl\"] = df_spl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba7d07-94b8-4863-a3e2-12f74bff9677",
   "metadata": {},
   "source": [
    "# 3) Load and preprocess SQLite tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "47043d8d-37f0-438a-8934-4852a55e8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "df_sql_tokens = pd.read_sql_query(\"\"\"\n",
    "    SELECT\n",
    "        timestamp,\n",
    "        open_usd,\n",
    "        high_usd,\n",
    "        low_usd,\n",
    "        close_usd,\n",
    "        volume_usd,\n",
    "        holder_count,\n",
    "        token_mint AS token,\n",
    "        token_name\n",
    "    FROM ohlcv_12h;\n",
    "\"\"\", conn)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df_sql_tokens[\"timestamp\"] = pd.to_datetime(df_sql_tokens[\"timestamp\"])\n",
    "\n",
    "dfs[\"sql_tokens\"] = df_sql_tokens\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e927976-1540-4bf2-bd0e-3fcef4493e8a",
   "metadata": {},
   "source": [
    "# 4) Load and preprocess each token OHLCV CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "bd927894-4c09-4c7a-8098-c1eb2acb9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_tokens = {}\n",
    "for token_path in token_files:\n",
    "    token_name = os.path.splitext(os.path.basename(token_path))[0]  # e.g. \"$WIF\"\n",
    "    df_tok = pd.read_csv(token_path)\n",
    "\n",
    "    # Drop the (empty) holder_count column if it exists in these per-token CSVs\n",
    "    if \"holder_count\" in df_tok.columns:\n",
    "        df_tok.drop(columns=[\"holder_count\"], inplace=True)\n",
    "\n",
    "    # Standardize timestamp column if named differently\n",
    "    if \"timestamp\" not in df_tok.columns:\n",
    "        for col in [\"time\", \"date\", \"datetime\"]:\n",
    "            if col in df_tok.columns:\n",
    "                df_tok.rename(columns={col: \"timestamp\"}, inplace=True)\n",
    "                break\n",
    "    df_tok[\"timestamp\"] = pd.to_datetime(df_tok[\"timestamp\"])\n",
    "\n",
    "    # Now df_tok has no 'holder_count'; its columns are [timestamp, open_usd, high_usd, low_usd, close_usd, volume_usd, ...]\n",
    "    dfs_tokens[token_name] = df_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89086b25-2975-4c2e-b548-1d306b9a2faf",
   "metadata": {},
   "source": [
    "# 5) Summarize all loaded DataFrames for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "86a884d9-af6b-4d09-a740-f3536da4bc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary of loaded DataFrames (auxiliary CSVs) ===\n",
      "\n",
      "--- btc_eth_price ---\n",
      "Shape      : (361, 9)\n",
      "Columns    : ['timestamp', 'btc_open', 'btc_high', 'btc_low', 'btc_close', 'eth_open', 'eth_high', 'eth_low', 'eth_close']\n",
      "Time range : 2024-12-05 12:00:00 to 2025-06-03 12:00:00\n",
      "\n",
      "--- holders ---\n",
      "Shape      : (5071, 3)\n",
      "Columns    : ['timestamp', 'token', 'holder_count']\n",
      "Time range : 2025-02-07 12:00:00+00:00 to 2025-06-03 11:00:00+00:00\n",
      "\n",
      "--- network_tx ---\n",
      "Shape      : (361, 2)\n",
      "Columns    : ['timestamp', 'tx_count']\n",
      "Time range : 2024-12-05 00:00:00+00:00 to 2025-06-03 00:00:00+00:00\n",
      "\n",
      "--- new_token_accounts ---\n",
      "Shape      : (7500, 3)\n",
      "Columns    : ['token', 'timestamp', 'new_token_accounts']\n",
      "Time range : 2024-12-05 00:00:00+00:00 to 2025-06-03 00:00:00+00:00\n",
      "\n",
      "--- sol_price ---\n",
      "Shape      : (361, 6)\n",
      "Columns    : ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
      "Time range : 2024-12-05 12:00:00 to 2025-06-03 12:00:00\n",
      "\n",
      "--- transfers ---\n",
      "Shape      : (7502, 3)\n",
      "Columns    : ['token', 'timestamp', 'transfer_count']\n",
      "Time range : 2024-12-05 12:00:00+00:00 to 2025-06-03 00:00:00+00:00\n",
      "\n",
      "--- tvl ---\n",
      "Shape      : (359, 3)\n",
      "Columns    : ['timestamp', 'tvl_usd', 'tvl_change_12h']\n",
      "Time range : 2024-12-06 00:00:00 to 2025-06-03 00:00:00\n",
      "\n",
      "--- spl ---\n",
      "Shape      : (360, 2)\n",
      "Columns    : ['timestamp', 'spl_instruction_count']\n",
      "Time range : 2024-12-05 12:00:00+00:00 to 2025-06-03 00:00:00+00:00\n",
      "\n",
      "--- sql_tokens ---\n",
      "Shape      : (7274, 9)\n",
      "Columns    : ['timestamp', 'open_usd', 'high_usd', 'low_usd', 'close_usd', 'volume_usd', 'holder_count', 'token', 'token_name']\n",
      "Time range : 2024-12-05 12:00:00 to 2025-06-03 12:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Summary of loaded DataFrames (auxiliary CSVs) ===\\n\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Shape      :\", df.shape)\n",
    "    print(\"Columns    :\", df.columns.tolist())\n",
    "    if \"timestamp\" in df.columns and not df[\"timestamp\"].isnull().all():\n",
    "        print(\"Time range :\", df[\"timestamp\"].min(), \"to\", df[\"timestamp\"].max())\n",
    "    else:\n",
    "        print(\"Time range : (no timestamp column or all null)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "549f15b7-a153-4b92-bfd3-afa42dba27bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample per-token CSVs (first 3 tokens) ===\n",
      "\n",
      "--- Token: $COLLAT ---\n",
      "Shape      : (241, 8)\n",
      "Columns    : ['timestamp', 'open_usd', 'high_usd', 'low_usd', 'close_usd', 'volume_usd', 'token_mint', 'token_name']\n",
      "Time range : 2025-02-03 12:00:00 to 2025-06-03 12:00:00\n",
      "\n",
      "--- Token: $michi ---\n",
      "Shape      : (301, 8)\n",
      "Columns    : ['timestamp', 'open_usd', 'high_usd', 'low_usd', 'close_usd', 'volume_usd', 'token_mint', 'token_name']\n",
      "Time range : 2025-01-04 12:00:00 to 2025-06-03 12:00:00\n",
      "\n",
      "--- Token: $WIF ---\n",
      "Shape      : (361, 8)\n",
      "Columns    : ['timestamp', 'open_usd', 'high_usd', 'low_usd', 'close_usd', 'volume_usd', 'token_mint', 'token_name']\n",
      "Time range : 2024-12-05 12:00:00 to 2025-06-03 12:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Sample per-token CSVs (first 3 tokens) ===\\n\")\n",
    "for tok, df_tok in list(dfs_tokens.items())[:3]:\n",
    "    print(f\"--- Token: {tok} ---\")\n",
    "    print(\"Shape      :\", df_tok.shape)\n",
    "    print(\"Columns    :\", df_tok.columns.tolist())\n",
    "    print(\"Time range :\", df_tok[\"timestamp\"].min(), \"to\", df_tok[\"timestamp\"].max())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488aa85-6681-4edc-8194-e1d205f5e645",
   "metadata": {},
   "source": [
    "# **Part 2: Master 12-hour index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "ec0c5fdb-4e6e-4c30-8d26-2e9a3072b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f299a28f-ead8-4aac-a8e6-caaf0b5dbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separately, we have loaded three token-specific CSVs that need the same “per-token” treatment:\n",
    "#       1. `holders_df` (columns: [\"timestamp\",\"token\",\"holder_count\"])\n",
    "#       2. `transfers_df` (columns: [\"timestamp\",\"token\",\"transfer_count\"])\n",
    "#       3. `new_accounts_df` (columns: [\"timestamp\",\"token\",\"new_accounts\"])\n",
    "\n",
    "# After this step, we will have:\n",
    "#  • aligned_globals[name]: each global series reindexed onto the 12h grid with forward-fill.\n",
    "#  • aligned_token_tables[token][“holders”], aligned_token_tables[token][“transfers”], aligned_token_tables[token][“new_accounts”]: each token’s series reindexed onto the 12h grid (NaN where missing).\n",
    "#  • aligned_tokens[token]: each token’s OHLCV from dfs_tokens reindexed onto the 12h grid (NaN where missing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "04b11965-168c-474e-b949-2321e6e47879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Normalize (tz-naive) and deduplicate GLOBAL tables\n",
    "# --------------------------------------------------\n",
    "global_names = [\"btc_eth_price\", \"sol_price\", \"network_tx\", \"tvl\"]\n",
    "for name in global_names:\n",
    "    df = dfs[name]\n",
    "    # 1) Convert timestamp to tz-naive\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "    # 2) Drop duplicate timestamps\n",
    "    df.sort_values(\"timestamp\", inplace=True)\n",
    "    df.drop_duplicates(subset=[\"timestamp\"], keep=\"last\", inplace=True)\n",
    "    # 3) Set timestamp index\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "809e2015-585d-4f36-9b69-b21de4c1d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Normalize and deduplicate token-specific tables\n",
    "#         (holders, new_token_accounts, transfers)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 2.2.a) holders_df\n",
    "df_holders = dfs[\"holders\"]\n",
    "df_holders[\"timestamp\"] = pd.to_datetime(df_holders[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "# token column already renamed\n",
    "df_holders.sort_values([\"token\", \"timestamp\"], inplace=True)\n",
    "df_holders.drop_duplicates(subset=[\"token\", \"timestamp\"], keep=\"last\", inplace=True)\n",
    "\n",
    "# 2.2.b) new_token_accounts_df\n",
    "df_newacc = dfs[\"new_token_accounts\"]\n",
    "df_newacc[\"timestamp\"] = pd.to_datetime(df_newacc[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "# token column already renamed\n",
    "df_newacc.sort_values([\"token\", \"timestamp\"], inplace=True)\n",
    "df_newacc.drop_duplicates(subset=[\"token\", \"timestamp\"], keep=\"last\", inplace=True)\n",
    "\n",
    "# 2.2.c) transfers_df\n",
    "df_transfers = dfs[\"transfers\"]\n",
    "df_transfers[\"timestamp\"] = pd.to_datetime(df_transfers[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "# token column already renamed\n",
    "df_transfers.sort_values([\"token\", \"timestamp\"], inplace=True)\n",
    "df_transfers.drop_duplicates(subset=[\"token\", \"timestamp\"], keep=\"last\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "2b8d8e1a-51b9-4216-af4b-a97b8cbfa0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Normalize & deduplicate per-token OHLCV CSVs\n",
    "# --------------------------------------------------\n",
    "for token, df_tok in dfs_tokens.items():\n",
    "    df_tok[\"timestamp\"] = pd.to_datetime(df_tok[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "    df_tok.sort_values(\"timestamp\", inplace=True)\n",
    "    df_tok.drop_duplicates(subset=[\"timestamp\"], keep=\"last\", inplace=True)\n",
    "    dfs_tokens[token] = df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "287ee6a8-7687-4718-8ec2-575dde48744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall data span: 2024-12-05 00:00:00 to 2025-06-03 12:00:00\n",
      "Master 12h index length: 362, from 2024-12-05 00:00:00 to 2025-06-03 12:00:00\n"
     ]
    }
   ],
   "source": [
    "# 2.4 Determine overall min/max timestamp\n",
    "# --------------------------------------------------\n",
    "min_times = []\n",
    "max_times = []\n",
    "\n",
    "# Global\n",
    "for name in global_names:\n",
    "    df = dfs[name]\n",
    "    if not df.index.empty:\n",
    "        min_times.append(df.index.min())\n",
    "        max_times.append(df.index.max())\n",
    "\n",
    "# Token-specific\n",
    "min_times.append(df_holders[\"timestamp\"].min())\n",
    "max_times.append(df_holders[\"timestamp\"].max())\n",
    "\n",
    "min_times.append(df_newacc[\"timestamp\"].min())\n",
    "max_times.append(df_newacc[\"timestamp\"].max())\n",
    "\n",
    "min_times.append(df_transfers[\"timestamp\"].min())\n",
    "max_times.append(df_transfers[\"timestamp\"].max())\n",
    "\n",
    "# Per-token OHLCV\n",
    "for df_tok in dfs_tokens.values():\n",
    "    if not df_tok[\"timestamp\"].isna().all():\n",
    "        min_times.append(df_tok[\"timestamp\"].min())\n",
    "        max_times.append(df_tok[\"timestamp\"].max())\n",
    "\n",
    "overall_start = min(min_times)\n",
    "overall_end   = max(max_times)\n",
    "print(f\"Overall data span: {overall_start} to {overall_end}\")\n",
    "\n",
    "# Build full 12h index\n",
    "full_12h_index = pd.date_range(start=overall_start, end=overall_end, freq=\"12h\")\n",
    "print(f\"Master 12h index length: {len(full_12h_index)}, from {full_12h_index.min()} to {full_12h_index.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "bf18b9ae-ed4b-4967-9cd1-50f23d8f6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Align GLOBAL tables onto full_12h_index (ffill)\n",
    "# --------------------------------------------------\n",
    "aligned_globals = {}\n",
    "for name in global_names:\n",
    "    df = dfs[name]\n",
    "    df_reindexed = (\n",
    "        df\n",
    "        .reindex(full_12h_index)  # introduce NaN rows\n",
    "        .ffill()                  # forward-fill\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"timestamp\"})\n",
    "    )\n",
    "    aligned_globals[name] = df_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "46db3eca-8a51-4c6c-a04c-a4382c20d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Align token-specific tables per token\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 2.6.a) holders per token\n",
    "aligned_holders = {}\n",
    "for token in df_holders[\"token\"].unique():\n",
    "    df_tok = df_holders[df_holders[\"token\"] == token].copy()\n",
    "    df_tok.set_index(\"timestamp\", inplace=True)\n",
    "    df_tok = df_tok[[\"holder_count\"]]\n",
    "    df_reindexed = (\n",
    "        df_tok\n",
    "        .reindex(full_12h_index)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"timestamp\"})\n",
    "    )\n",
    "    df_reindexed[\"token\"] = token\n",
    "    aligned_holders[token] = df_reindexed\n",
    "\n",
    "# 2.6.b) new_token_accounts per token\n",
    "aligned_new_accounts = {}\n",
    "for token in df_newacc[\"token\"].unique():\n",
    "    df_tok = df_newacc[df_newacc[\"token\"] == token].copy()\n",
    "    df_tok.set_index(\"timestamp\", inplace=True)\n",
    "    df_tok = df_tok[[\"new_token_accounts\"]]\n",
    "    df_reindexed = (\n",
    "        df_tok\n",
    "        .reindex(full_12h_index)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"timestamp\"})\n",
    "    )\n",
    "    df_reindexed[\"token\"] = token\n",
    "    aligned_new_accounts[token] = df_reindexed\n",
    "\n",
    "# 2.6.c) transfers per token\n",
    "aligned_transfers = {}\n",
    "for token in df_transfers[\"token\"].unique():\n",
    "    df_tok = df_transfers[df_transfers[\"token\"] == token].copy()\n",
    "    df_tok.set_index(\"timestamp\", inplace=True)\n",
    "    df_tok = df_tok[[\"transfer_count\"]]\n",
    "    df_reindexed = (\n",
    "        df_tok\n",
    "        .reindex(full_12h_index)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"timestamp\"})\n",
    "    )\n",
    "    df_reindexed[\"token\"] = token\n",
    "    aligned_transfers[token] = df_reindexed\n",
    "\n",
    "# 2.6.d) OHLCV CSVs per token\n",
    "aligned_tokens_ohlcv = {}\n",
    "for token, df_tok in dfs_tokens.items():\n",
    "    df_tok.set_index(\"timestamp\", inplace=True)\n",
    "    df_reindexed = (\n",
    "        df_tok\n",
    "        .reindex(full_12h_index)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"timestamp\"})\n",
    "    )\n",
    "    df_reindexed[\"token\"] = token\n",
    "    aligned_tokens_ohlcv[token] = df_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "527496e5-65b9-4a9e-b148-3c6e8dd41708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aligned global: btc_eth_price (head) ---\n",
      "            timestamp       btc_open       btc_high       btc_low  \\\n",
      "0 2024-12-05 00:00:00            NaN            NaN           NaN   \n",
      "1 2024-12-05 12:00:00  103036.856923  103606.802840  96489.621485   \n",
      "2 2024-12-06 00:00:00   97071.794751   98400.883900  97071.794751   \n",
      "3 2024-12-06 12:00:00   98113.436509  101590.521373  97830.483957   \n",
      "4 2024-12-07 00:00:00   99928.055215  100408.745127  99260.728938   \n",
      "\n",
      "       btc_close     eth_open     eth_high      eth_low    eth_close  \n",
      "0            NaN          NaN          NaN          NaN          NaN  \n",
      "1   96489.621485  3934.972713  3940.376136  3806.624693  3806.624693  \n",
      "2   98235.614141  3791.374120  3914.346072  3791.374120  3872.775968  \n",
      "3  100648.833175  3867.363728  4067.577452  3854.354317  4031.584105  \n",
      "4   99639.933180  4007.725653  4018.369292  3986.505629  3986.815519  \n",
      "\n",
      "--- Aligned holders for token 2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump (first 3) ---\n",
      "            timestamp  holder_count  \\\n",
      "0 2024-12-05 00:00:00           NaN   \n",
      "1 2024-12-05 12:00:00           NaN   \n",
      "2 2024-12-06 00:00:00           NaN   \n",
      "\n",
      "                                          token  \n",
      "0  2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump  \n",
      "1  2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump  \n",
      "2  2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump  \n",
      "\n",
      "--- Aligned new_accounts for token 2qehjdldlbubgryvsxhc5d6udwaivnfzgan56p1tpump (first 3) ---\n",
      "            timestamp  new_token_accounts  \\\n",
      "0 2024-12-05 00:00:00                96.0   \n",
      "1 2024-12-05 12:00:00              2072.0   \n",
      "2 2024-12-06 00:00:00              1049.0   \n",
      "\n",
      "                                          token  \n",
      "0  2qehjdldlbubgryvsxhc5d6udwaivnfzgan56p1tpump  \n",
      "1  2qehjdldlbubgryvsxhc5d6udwaivnfzgan56p1tpump  \n",
      "2  2qehjdldlbubgryvsxhc5d6udwaivnfzgan56p1tpump  \n",
      "\n",
      "--- Aligned transfers for token 2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump (first 3) ---\n",
      "            timestamp  transfer_count  \\\n",
      "0 2024-12-05 00:00:00             NaN   \n",
      "1 2024-12-05 12:00:00         88901.0   \n",
      "2 2024-12-06 00:00:00         51091.0   \n",
      "\n",
      "                                          token  \n",
      "0  2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump  \n",
      "1  2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump  \n",
      "2  2qEHjDLDLbuBgRYvsxhc5D6uDWAivNFZGan56P1tpump  \n",
      "\n",
      "--- Aligned OHLCV for token $COLLAT (first 3) ---\n",
      "            timestamp  open_usd  high_usd  low_usd  close_usd  volume_usd  \\\n",
      "0 2024-12-05 00:00:00       NaN       NaN      NaN        NaN         NaN   \n",
      "1 2024-12-05 12:00:00       NaN       NaN      NaN        NaN         NaN   \n",
      "2 2024-12-06 00:00:00       NaN       NaN      NaN        NaN         NaN   \n",
      "\n",
      "  token_mint token_name    token  \n",
      "0        NaN        NaN  $COLLAT  \n",
      "1        NaN        NaN  $COLLAT  \n",
      "2        NaN        NaN  $COLLAT  \n"
     ]
    }
   ],
   "source": [
    "# 2.7 Sanity Check\n",
    "# --------------------------------------------------\n",
    "print(\"\\n--- Aligned global: btc_eth_price (head) ---\")\n",
    "print(aligned_globals[\"btc_eth_price\"].head())\n",
    "\n",
    "example_holder = next(iter(aligned_holders))\n",
    "print(f\"\\n--- Aligned holders for token {example_holder} (first 3) ---\")\n",
    "print(aligned_holders[example_holder].head(3))\n",
    "\n",
    "example_newacc = next(iter(aligned_new_accounts))\n",
    "print(f\"\\n--- Aligned new_accounts for token {example_newacc} (first 3) ---\")\n",
    "print(aligned_new_accounts[example_newacc].head(3))\n",
    "\n",
    "example_transfer = next(iter(aligned_transfers))\n",
    "print(f\"\\n--- Aligned transfers for token {example_transfer} (first 3) ---\")\n",
    "print(aligned_transfers[example_transfer].head(3))\n",
    "\n",
    "example_ohlcv = next(iter(aligned_tokens_ohlcv))\n",
    "print(f\"\\n--- Aligned OHLCV for token {example_ohlcv} (first 3) ---\")\n",
    "print(aligned_tokens_ohlcv[example_ohlcv].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1663c9-0170-4198-be61-8a53bfc9b036",
   "metadata": {},
   "source": [
    "#  Step 3: Merge All Aligned DataFrames into a Master Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "0d468be5-a7c6-4e29-a8b3-afb94c81bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Concatenate Token-Level Data\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 3.1.a) OHLCV data for all tokens\n",
    "# aligned_tokens_ohlcv is a dict: {token: DataFrame with columns [timestamp, open_usd, high_usd, low_usd, close_usd, volume_usd, token]}\n",
    "df_ohlcv_master = pd.concat(aligned_tokens_ohlcv.values(), ignore_index=True)\n",
    "\n",
    "# 3.1.b) Holder counts for all tokens\n",
    "# aligned_holders is a dict: {token: DataFrame with columns [timestamp, holder_count, token]}\n",
    "df_holders_master = pd.concat(aligned_holders.values(), ignore_index=True)\n",
    "\n",
    "# 3.1.c) New token accounts for all tokens\n",
    "# aligned_new_accounts is a dict: {token: DataFrame with columns [timestamp, new_token_accounts, token]}\n",
    "df_newacc_master = pd.concat(aligned_new_accounts.values(), ignore_index=True)\n",
    "\n",
    "# 3.1.d) Transfer counts for all tokens\n",
    "# aligned_transfers is a dict: {token: DataFrame with columns [timestamp, transfer_count, token]}\n",
    "df_transfers_master = pd.concat(aligned_transfers.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "20d85d16-038e-4850-9464-1394e2c73ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Merge Token-Level Tables\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Start from OHLCV as the base\n",
    "df_tokens_combined = (\n",
    "    df_ohlcv_master\n",
    "    .merge(\n",
    "        df_holders_master[['token', 'timestamp', 'holder_count']],\n",
    "        on=['token', 'timestamp'],\n",
    "        how='left'\n",
    "    )\n",
    "    .merge(\n",
    "        df_newacc_master[['token', 'timestamp', 'new_token_accounts']],\n",
    "        on=['token', 'timestamp'],\n",
    "        how='left'\n",
    "    )\n",
    "    .merge(\n",
    "        df_transfers_master[['token', 'timestamp', 'transfer_count']],\n",
    "        on=['token', 'timestamp'],\n",
    "        how='left'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a0b52f7e-23a0-479c-918a-2fc7cc3daab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Merge Global Features\n",
    "# --------------------------------------------------\n",
    "\n",
    "# aligned_globals is a dict where each value is a DataFrame with columns [timestamp, <global_feature_columns>]\n",
    "df_master = df_tokens_combined.copy()\n",
    "\n",
    "# Loop over each global dataset to merge on timestamp\n",
    "for name, df_glob in aligned_globals.items():\n",
    "    # Rename global columns (except \"timestamp\") by prefixing with dataset name\n",
    "    cols_to_rename = {col: f\"{name}_{col}\" for col in df_glob.columns if col != \"timestamp\"}\n",
    "    df_glob_renamed = df_glob.rename(columns=cols_to_rename)\n",
    "    \n",
    "    # Merge into master panel\n",
    "    df_master = df_master.merge(\n",
    "        df_glob_renamed,\n",
    "        on=\"timestamp\",\n",
    "        how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "6c778983-3c26-448f-8bce-7d6e27effb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Reorder Columns (Optional)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Place timestamp and token first, then token-level features, then global features\n",
    "token_cols = [\n",
    "    \"timestamp\", \"token\",\n",
    "    \"open_usd\", \"high_usd\", \"low_usd\", \"close_usd\", \"volume_usd\",\n",
    "    \"holder_count\", \"new_token_accounts\", \"transfer_count\"\n",
    "]\n",
    "\n",
    "global_cols = [col for col in df_master.columns if col not in token_cols]\n",
    "ordered_cols = token_cols + global_cols\n",
    "df_master = df_master[ordered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d4a76f82-d554-42bd-b967-ac80ab059aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp    token  open_usd  high_usd  low_usd  close_usd  \\\n",
      "0 2024-12-05 00:00:00  $COLLAT       NaN       NaN      NaN        NaN   \n",
      "1 2024-12-05 12:00:00  $COLLAT       NaN       NaN      NaN        NaN   \n",
      "2 2024-12-06 00:00:00  $COLLAT       NaN       NaN      NaN        NaN   \n",
      "3 2024-12-06 12:00:00  $COLLAT       NaN       NaN      NaN        NaN   \n",
      "4 2024-12-07 00:00:00  $COLLAT       NaN       NaN      NaN        NaN   \n",
      "\n",
      "   volume_usd  holder_count  new_token_accounts  transfer_count  ...  \\\n",
      "0         NaN           NaN                 NaN             NaN  ...   \n",
      "1         NaN           NaN                 NaN             NaN  ...   \n",
      "2         NaN           NaN                 NaN             NaN  ...   \n",
      "3         NaN           NaN                 NaN             NaN  ...   \n",
      "4         NaN           NaN                 NaN             NaN  ...   \n",
      "\n",
      "  btc_eth_price_eth_low btc_eth_price_eth_close  sol_price_open  \\\n",
      "0                   NaN                     NaN             NaN   \n",
      "1           3806.624693             3806.624693      242.483567   \n",
      "2           3791.374120             3872.775968      236.882044   \n",
      "3           3854.354317             4031.584105      234.225689   \n",
      "4           3986.505629             3986.815519      237.089680   \n",
      "\n",
      "   sol_price_high  sol_price_low  sol_price_close  sol_price_volume  \\\n",
      "0             NaN            NaN              NaN               NaN   \n",
      "1      242.483567     232.376240       233.289091       2611.759253   \n",
      "2      244.364756     235.805695       236.044413       2873.570727   \n",
      "3      240.084565     232.639141       238.073414       2852.116897   \n",
      "4      238.549845     235.117627       238.438472       2837.946730   \n",
      "\n",
      "   network_tx_tx_count   tvl_tvl_usd  tvl_tvl_change_12h  \n",
      "0             866531.0           NaN                 NaN  \n",
      "1          182550905.0           NaN                 NaN  \n",
      "2          183188078.0  1.941217e+10            0.028605  \n",
      "3          183446143.0  1.941217e+10            0.000000  \n",
      "4          183675936.0  1.971885e+10            0.015798  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "Columns in Master Panel:\n",
      "['timestamp', 'token', 'open_usd', 'high_usd', 'low_usd', 'close_usd', 'volume_usd', 'holder_count', 'new_token_accounts', 'transfer_count', 'token_mint', 'token_name', 'btc_eth_price_btc_open', 'btc_eth_price_btc_high', 'btc_eth_price_btc_low', 'btc_eth_price_btc_close', 'btc_eth_price_eth_open', 'btc_eth_price_eth_high', 'btc_eth_price_eth_low', 'btc_eth_price_eth_close', 'sol_price_open', 'sol_price_high', 'sol_price_low', 'sol_price_close', 'sol_price_volume', 'network_tx_tx_count', 'tvl_tvl_usd', 'tvl_tvl_change_12h']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 3.5 Inspect the Master Panel\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(df_master.head())\n",
    "print(\"\\nColumns in Master Panel:\")\n",
    "print(df_master.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "36e631bc-ee79-43fa-9b67-578a00f9ebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>token</th>\n",
       "      <th>open_usd</th>\n",
       "      <th>high_usd</th>\n",
       "      <th>low_usd</th>\n",
       "      <th>close_usd</th>\n",
       "      <th>volume_usd</th>\n",
       "      <th>holder_count</th>\n",
       "      <th>new_token_accounts</th>\n",
       "      <th>transfer_count</th>\n",
       "      <th>...</th>\n",
       "      <th>btc_eth_price_eth_low</th>\n",
       "      <th>btc_eth_price_eth_close</th>\n",
       "      <th>sol_price_open</th>\n",
       "      <th>sol_price_high</th>\n",
       "      <th>sol_price_low</th>\n",
       "      <th>sol_price_close</th>\n",
       "      <th>sol_price_volume</th>\n",
       "      <th>network_tx_tx_count</th>\n",
       "      <th>tvl_tvl_usd</th>\n",
       "      <th>tvl_tvl_change_12h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-05 00:00:00</td>\n",
       "      <td>$COLLAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>866531.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-05 12:00:00</td>\n",
       "      <td>$COLLAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3806.624693</td>\n",
       "      <td>3806.624693</td>\n",
       "      <td>242.483567</td>\n",
       "      <td>242.483567</td>\n",
       "      <td>232.376240</td>\n",
       "      <td>233.289091</td>\n",
       "      <td>2611.759253</td>\n",
       "      <td>182550905.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-06 00:00:00</td>\n",
       "      <td>$COLLAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3791.374120</td>\n",
       "      <td>3872.775968</td>\n",
       "      <td>236.882044</td>\n",
       "      <td>244.364756</td>\n",
       "      <td>235.805695</td>\n",
       "      <td>236.044413</td>\n",
       "      <td>2873.570727</td>\n",
       "      <td>183188078.0</td>\n",
       "      <td>1.941217e+10</td>\n",
       "      <td>0.028605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-06 12:00:00</td>\n",
       "      <td>$COLLAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3854.354317</td>\n",
       "      <td>4031.584105</td>\n",
       "      <td>234.225689</td>\n",
       "      <td>240.084565</td>\n",
       "      <td>232.639141</td>\n",
       "      <td>238.073414</td>\n",
       "      <td>2852.116897</td>\n",
       "      <td>183446143.0</td>\n",
       "      <td>1.941217e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-07 00:00:00</td>\n",
       "      <td>$COLLAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3986.505629</td>\n",
       "      <td>3986.815519</td>\n",
       "      <td>237.089680</td>\n",
       "      <td>238.549845</td>\n",
       "      <td>235.117627</td>\n",
       "      <td>238.438472</td>\n",
       "      <td>2837.946730</td>\n",
       "      <td>183675936.0</td>\n",
       "      <td>1.971885e+10</td>\n",
       "      <td>0.015798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8321</th>\n",
       "      <td>2025-06-01 12:00:00</td>\n",
       "      <td>ZEREBRO</td>\n",
       "      <td>0.029810</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.029327</td>\n",
       "      <td>0.035637</td>\n",
       "      <td>0.390691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2484.639706</td>\n",
       "      <td>2538.239570</td>\n",
       "      <td>152.103430</td>\n",
       "      <td>156.893238</td>\n",
       "      <td>151.033222</td>\n",
       "      <td>156.893238</td>\n",
       "      <td>1855.822749</td>\n",
       "      <td>176590346.0</td>\n",
       "      <td>1.909351e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>2025-06-02 00:00:00</td>\n",
       "      <td>ZEREBRO</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.036552</td>\n",
       "      <td>0.031606</td>\n",
       "      <td>0.031826</td>\n",
       "      <td>0.403126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2482.897102</td>\n",
       "      <td>2483.965452</td>\n",
       "      <td>157.678225</td>\n",
       "      <td>158.278749</td>\n",
       "      <td>153.713839</td>\n",
       "      <td>153.966326</td>\n",
       "      <td>1872.837072</td>\n",
       "      <td>172739102.0</td>\n",
       "      <td>1.918176e+10</td>\n",
       "      <td>0.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8323</th>\n",
       "      <td>2025-06-02 12:00:00</td>\n",
       "      <td>ZEREBRO</td>\n",
       "      <td>0.031822</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.030969</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.384153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2483.016657</td>\n",
       "      <td>2581.057455</td>\n",
       "      <td>153.718959</td>\n",
       "      <td>156.300049</td>\n",
       "      <td>152.574077</td>\n",
       "      <td>156.300049</td>\n",
       "      <td>1845.678881</td>\n",
       "      <td>179729332.0</td>\n",
       "      <td>1.918176e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>2025-06-03 00:00:00</td>\n",
       "      <td>ZEREBRO</td>\n",
       "      <td>0.032649</td>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.032649</td>\n",
       "      <td>0.033673</td>\n",
       "      <td>0.404068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2596.048973</td>\n",
       "      <td>2614.005891</td>\n",
       "      <td>156.878012</td>\n",
       "      <td>161.951983</td>\n",
       "      <td>156.878012</td>\n",
       "      <td>161.158466</td>\n",
       "      <td>1914.444299</td>\n",
       "      <td>160288119.0</td>\n",
       "      <td>1.952288e+10</td>\n",
       "      <td>0.017783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8325</th>\n",
       "      <td>2025-06-03 12:00:00</td>\n",
       "      <td>ZEREBRO</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2606.736401</td>\n",
       "      <td>2606.736401</td>\n",
       "      <td>160.839439</td>\n",
       "      <td>160.839439</td>\n",
       "      <td>160.839439</td>\n",
       "      <td>160.839439</td>\n",
       "      <td>160.839439</td>\n",
       "      <td>160288119.0</td>\n",
       "      <td>1.952288e+10</td>\n",
       "      <td>0.017783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8326 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp    token  open_usd  high_usd   low_usd  close_usd  \\\n",
       "0    2024-12-05 00:00:00  $COLLAT       NaN       NaN       NaN        NaN   \n",
       "1    2024-12-05 12:00:00  $COLLAT       NaN       NaN       NaN        NaN   \n",
       "2    2024-12-06 00:00:00  $COLLAT       NaN       NaN       NaN        NaN   \n",
       "3    2024-12-06 12:00:00  $COLLAT       NaN       NaN       NaN        NaN   \n",
       "4    2024-12-07 00:00:00  $COLLAT       NaN       NaN       NaN        NaN   \n",
       "...                  ...      ...       ...       ...       ...        ...   \n",
       "8321 2025-06-01 12:00:00  ZEREBRO  0.029810  0.037385  0.029327   0.035637   \n",
       "8322 2025-06-02 00:00:00  ZEREBRO  0.036545  0.036552  0.031606   0.031826   \n",
       "8323 2025-06-02 12:00:00  ZEREBRO  0.031822  0.032946  0.030969   0.032946   \n",
       "8324 2025-06-03 00:00:00  ZEREBRO  0.032649  0.034154  0.032649   0.033673   \n",
       "8325 2025-06-03 12:00:00  ZEREBRO  0.033312  0.033312  0.033312   0.033312   \n",
       "\n",
       "      volume_usd  holder_count  new_token_accounts  transfer_count  ...  \\\n",
       "0            NaN           NaN                 NaN             NaN  ...   \n",
       "1            NaN           NaN                 NaN             NaN  ...   \n",
       "2            NaN           NaN                 NaN             NaN  ...   \n",
       "3            NaN           NaN                 NaN             NaN  ...   \n",
       "4            NaN           NaN                 NaN             NaN  ...   \n",
       "...          ...           ...                 ...             ...  ...   \n",
       "8321    0.390691           NaN                 NaN             NaN  ...   \n",
       "8322    0.403126           NaN                 NaN             NaN  ...   \n",
       "8323    0.384153           NaN                 NaN             NaN  ...   \n",
       "8324    0.404068           NaN                 NaN             NaN  ...   \n",
       "8325    0.033312           NaN                 NaN             NaN  ...   \n",
       "\n",
       "     btc_eth_price_eth_low btc_eth_price_eth_close  sol_price_open  \\\n",
       "0                      NaN                     NaN             NaN   \n",
       "1              3806.624693             3806.624693      242.483567   \n",
       "2              3791.374120             3872.775968      236.882044   \n",
       "3              3854.354317             4031.584105      234.225689   \n",
       "4              3986.505629             3986.815519      237.089680   \n",
       "...                    ...                     ...             ...   \n",
       "8321           2484.639706             2538.239570      152.103430   \n",
       "8322           2482.897102             2483.965452      157.678225   \n",
       "8323           2483.016657             2581.057455      153.718959   \n",
       "8324           2596.048973             2614.005891      156.878012   \n",
       "8325           2606.736401             2606.736401      160.839439   \n",
       "\n",
       "      sol_price_high  sol_price_low  sol_price_close  sol_price_volume  \\\n",
       "0                NaN            NaN              NaN               NaN   \n",
       "1         242.483567     232.376240       233.289091       2611.759253   \n",
       "2         244.364756     235.805695       236.044413       2873.570727   \n",
       "3         240.084565     232.639141       238.073414       2852.116897   \n",
       "4         238.549845     235.117627       238.438472       2837.946730   \n",
       "...              ...            ...              ...               ...   \n",
       "8321      156.893238     151.033222       156.893238       1855.822749   \n",
       "8322      158.278749     153.713839       153.966326       1872.837072   \n",
       "8323      156.300049     152.574077       156.300049       1845.678881   \n",
       "8324      161.951983     156.878012       161.158466       1914.444299   \n",
       "8325      160.839439     160.839439       160.839439        160.839439   \n",
       "\n",
       "      network_tx_tx_count   tvl_tvl_usd  tvl_tvl_change_12h  \n",
       "0                866531.0           NaN                 NaN  \n",
       "1             182550905.0           NaN                 NaN  \n",
       "2             183188078.0  1.941217e+10            0.028605  \n",
       "3             183446143.0  1.941217e+10            0.000000  \n",
       "4             183675936.0  1.971885e+10            0.015798  \n",
       "...                   ...           ...                 ...  \n",
       "8321          176590346.0  1.909351e+10            0.000000  \n",
       "8322          172739102.0  1.918176e+10            0.004622  \n",
       "8323          179729332.0  1.918176e+10            0.000000  \n",
       "8324          160288119.0  1.952288e+10            0.017783  \n",
       "8325          160288119.0  1.952288e+10            0.017783  \n",
       "\n",
       "[8326 rows x 28 columns]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
